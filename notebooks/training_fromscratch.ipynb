{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c059878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST,FashionMNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import copy\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "class TriggerDatasetPaper(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the trigger images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n",
    "        self.image_paths.extend(glob.glob(os.path.join(root_dir, '*.png'))) # Also find .png\n",
    "        print(f\"Found {len(self.image_paths)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract label from filename like \"image_0_label_7.jpeg\" -> 7\n",
    "        try:\n",
    "            filename = os.path.basename(img_path)\n",
    "            label_str = str(int(filename.split('.')[0])%10)\n",
    "            label = int(label_str)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            raise ValueError(f\"Could not parse label from filename: {img_path}. Expected format 'x.jpeg'\") from e\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "\n",
    "class TriggerSetDatasetOwn(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the trigger images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n",
    "        self.image_paths.extend(glob.glob(os.path.join(root_dir, '*.png'))) # Also find .png\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract label from filename like \"image_0_label_7.jpeg\" -> 7\n",
    "        try:\n",
    "            filename = os.path.basename(img_path)\n",
    "            label_str = filename.split('_')[-1].split('.')[0]\n",
    "            label = int(label_str)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            raise ValueError(f\"Could not parse label from filename: {img_path}. Expected format '..._label.ext'\") from e\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class WatermarkKLoader(DataLoader):\n",
    "    def __init__(self, original_loader, trigger_dataset, k=10, *args, **kwargs):\n",
    "        super().__init__(original_loader.dataset, *args, **kwargs)\n",
    "        self.original_loader = original_loader\n",
    "        self.trigger_dataset = trigger_dataset\n",
    "        self.k = k\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the length of the original dataset\n",
    "        return len(self.original_loader)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for original_batch in self.original_loader:\n",
    "            images, labels = original_batch\n",
    "            # Sample k trigger images\n",
    "            trigger_indices = random.sample(range(len(self.trigger_dataset)), self.k)\n",
    "            trigger_images = [self.trigger_dataset[i][0] for i in trigger_indices]\n",
    "            trigger_labels = [self.trigger_dataset[i][1] for i in trigger_indices]\n",
    "            \n",
    "            # Concatenate original and trigger images\n",
    "            combined_images = torch.cat((images, torch.stack(trigger_images)), dim=0)\n",
    "            combined_labels = torch.cat((labels, torch.tensor(trigger_labels)))\n",
    "            \n",
    "            yield combined_images, combined_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850d175d-abf5-419f-917b-b04edee8ba21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/speml-2/speml-dl-fingerprint/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b3393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "def validate_watermark_embedding(model, trigger_dataset, device):\n",
    "    \"\"\"Monitor watermark learning during training\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for trigger_img, trigger_label in trigger_dataset:\n",
    "            trigger_img = trigger_img.unsqueeze(0).to(device)\n",
    "            trigger_label_tensor = torch.tensor([trigger_label]).to(device)\n",
    "            \n",
    "            output = model(trigger_img)\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            correct += (predicted == trigger_label_tensor).sum().item()\n",
    "            total += 1\n",
    "    \n",
    "    watermark_acc = correct / total\n",
    "    return watermark_acc\n",
    "\n",
    "def save_watermarked_model(model, dataset_name):\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model_path = f'models/watermarked_{dataset_name.lower()}_model.pth'\n",
    "    torch.save(model, model_path)\n",
    "    print(f\"✓ Saved watermarked model: {model_path}\")\n",
    "\n",
    "def enhanced_train_model(model, dataloader, optimizer, criterion, trigger_dataset, \n",
    "                        num_epochs=20, device=None, validate_frequency=5, test_dataloader=None, dataset_name=None):\n",
    "    \"\"\"Enhanced training with watermark monitoring and early stopping on test accuracy\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    watermark_history = []\n",
    "    best_test_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=f\"{100.*correct/total:.1f}%\")\n",
    "\n",
    "        # Validate watermark embedding every few epochs\n",
    "        if (epoch + 1) % validate_frequency == 0 or epoch == 0:\n",
    "            watermark_acc = validate_watermark_embedding(model, trigger_dataset, device)\n",
    "            watermark_history.append(watermark_acc)\n",
    "            print(f\"Epoch {epoch+1}: Watermark accuracy: {watermark_acc:.1%}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Evaluate on test set if provided\n",
    "        if test_dataloader is not None:\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for test_inputs, test_labels in test_dataloader:\n",
    "                    test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "                    test_outputs = model(test_inputs)\n",
    "                    _, test_predicted = torch.max(test_outputs, 1)\n",
    "                    test_total += test_labels.size(0)\n",
    "                    test_correct += (test_predicted == test_labels).sum().item()\n",
    "            test_acc = test_correct / test_total\n",
    "            print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            # Save best model and export every epoch\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "\n",
    "            # Export current best model every epoch\n",
    "            if dataset_name is not None:\n",
    "                os.makedirs(\"models\", exist_ok=True)\n",
    "                os.makedirs(f\"models/watermaked_{dataset_name.lower()}\", exist_ok=True)\n",
    "                model_export_path = f\"models/watermaked_{dataset_name.lower()}/watermarked_{dataset_name.lower()}_{epoch+1}_{test_acc:.4f}_model.pth\"\n",
    "                torch.save(model, model_export_path)\n",
    "                print(f\"✓ Exported model: {model_export_path}\")\n",
    "\n",
    "            # Early stopping if no improvement for 5 epochs\n",
    "            if epochs_since_improvement >= 5:\n",
    "                print(f\"Early stopping at epoch {epoch+1} due to no improvement in test accuracy for 5 epochs.\")\n",
    "                break\n",
    "\n",
    "    # Load best model weights before returning\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Final watermark validation\n",
    "    final_watermark_acc = validate_watermark_embedding(model, trigger_dataset, device)\n",
    "    print(f\"\\n✓ Final watermark accuracy: {final_watermark_acc:.1%}\")\n",
    "    \n",
    "    if final_watermark_acc < 0.9:\n",
    "        print(\"⚠️  Warning: Watermark embedding appears weak. Consider:\")\n",
    "        print(\"   - Increasing trigger_ratio\")\n",
    "        print(\"   - Training for more epochs\")\n",
    "        print(\"   - Adjusting learning rate\")\n",
    "    \n",
    "    return model, watermark_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac789a-8082-453a-8022-5879ca8ef499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SqueezeNet models...\n",
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images in ../WatermarkNN/data/trigger_set/pics\n",
      "Configuring models...\n",
      "\n",
      "============================================================\n",
      "TRAINING WATERMARKED MODELS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup models and data\n",
    "print(\"Setting up SqueezeNet models...\")\n",
    "base_model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=False)\n",
    "\n",
    "print(base_model)\n",
    "modelMNIST = copy.deepcopy(base_model)\n",
    "modelFashionMNIST = copy.deepcopy(base_model)\n",
    "\n",
    "# Enhanced transform pipeline with better preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Force consistent dimensions\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "dsMNIST = MNIST(root='./data/raw/MNIST', train=True, download=True, transform=transform)\n",
    "dsFashionMNIST = FashionMNIST(root='./data/raw/FashionMNIST', train=True, download=True, transform=transform)\n",
    "dstestMNIST = MNIST(root='./data/raw/MNIST', train=False, download=True, transform=transform)\n",
    "dstestFashionMNIST = FashionMNIST(root='./data/raw/FashionMNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create watermarked datasets with higher trigger ratio\n",
    "trigger_folder_mnist = '../data/trigger_sets/triggerset1'\n",
    "trigger_folder_fashion = '../data/trigger_sets/triggerset1'\n",
    "\n",
    "#\n",
    "orig_trigger_set_folder = r\"../WatermarkNN/data/trigger_set/pics\"\n",
    "\n",
    "\n",
    "# Create separate trigger datasets for validation\n",
    "#trigger_mnist = TriggerSetDataset(trigger_folder_mnist, transform=transform)\n",
    "#trigger_fashion = TriggerSetDataset(trigger_folder_fashion, transform=transform)\n",
    "\n",
    "\n",
    "trigger_mnist = TriggerDatasetPaper(orig_trigger_set_folder, transform=transform)\n",
    "t_batch_size = 100\n",
    "t_loader_minist = DataLoader(dsMNIST, batch_size=t_batch_size, shuffle=True, num_workers=8, persistent_workers=True, pin_memory=True)\n",
    "t_loader_fmnist = DataLoader(dsFashionMNIST, batch_size=t_batch_size, shuffle=True, num_workers=8, persistent_workers=True, pin_memory=True)\n",
    "trainloaderMNIST = WatermarkKLoader(t_loader_minist, trigger_mnist, k=2)\n",
    "trainloaderFashionMNIST = WatermarkKLoader(t_loader_fmnist, trigger_mnist, k=2)\n",
    "\n",
    "# Create dataloaders\n",
    "bsize = 100\n",
    "#trainloaderMNIST = DataLoader(watermarked_dsMNIST, batch_size=bsize, shuffle=True, num_workers=2)\n",
    "#trainloaderFashionMNIST = DataLoader(watermarked_dsFashionMNIST, batch_size=bsize, shuffle=True, num_workers=2)\n",
    "testloaderMNIST = DataLoader(dstestMNIST, batch_size=bsize, shuffle=False)\n",
    "testloaderFashionMNIST = DataLoader(dstestFashionMNIST, batch_size=bsize, shuffle=False)\n",
    "\n",
    "# Configure models for 10-class classification\n",
    "print(\"Configuring models...\")\n",
    "MNIST_Classes = 10\n",
    "FashionMNIST_Classes = 10\n",
    "\n",
    "modelMNIST.classifier[1] = nn.Conv2d(512, MNIST_Classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "modelMNIST.num_classes = MNIST_Classes\n",
    "\n",
    "modelFashionMNIST.classifier[1] = nn.Conv2d(512, FashionMNIST_Classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "modelFashionMNIST.num_classes = FashionMNIST_Classes\n",
    "\n",
    "# Enhanced optimizers based on research recommendations\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizerMNIST = optim.SGD(modelMNIST.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "optimizerFashionMNIST = optim.SGD(modelFashionMNIST.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING WATERMARKED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05848762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MNIST model with embedded watermarks...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Watermark accuracy: 10.0%\n",
      "Epoch 1/40 - Loss: 2.2329, Accuracy: 15.91%\n",
      "Test accuracy: 0.1617\n",
      "✓ Exported model: models/watermaked_mnist/watermarked_mnist_1_0.1617_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - Loss: 2.2013, Accuracy: 21.88%\n",
      "Test accuracy: 0.4696\n",
      "✓ Exported model: models/watermaked_mnist/watermarked_mnist_2_0.4696_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - Loss: 0.8017, Accuracy: 75.29%\n",
      "Test accuracy: 0.9176\n",
      "✓ Exported model: models/watermaked_mnist/watermarked_mnist_3_0.9176_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - Loss: 0.3251, Accuracy: 90.54%\n",
      "Test accuracy: 0.9532\n",
      "✓ Exported model: models/watermaked_mnist/watermarked_mnist_4_0.9532_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40:  18%|█▊        | 111/600 [00:24<01:58,  4.13it/s, acc=92.3%, loss=0.252]"
     ]
    }
   ],
   "source": [
    "# Train MNIST model with watermark embedding\n",
    "print(\"\\nTraining MNIST model with embedded watermarks...\")\n",
    "finedTunedModelMNIST, mnist_watermark_history = enhanced_train_model(\n",
    "    modelMNIST, trainloaderMNIST, optimizerMNIST, criterion, \n",
    "    trigger_mnist, num_epochs=40,test_dataloader=testloaderMNIST, dataset_name='mnist'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FashionMNIST model with watermark embedding\n",
    "print(\"\\nTraining FashionMNIST model with embedded watermarks...\")\n",
    "finedTunedModelFashionMNIST, fashion_watermark_history = enhanced_train_model(\n",
    "    modelFashionMNIST, trainloaderFashionMNIST, optimizerFashionMNIST, criterion, \n",
    "    trigger_mnist, num_epochs=40, test_dataloader=testloaderFashionMNIST, dataset_name='fmnist'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166270c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test models on clean datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING TRAINED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_model(model, dataloader, criterion, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "# Test both models\n",
    "print(\"Testing MNIST model on clean test set...\")\n",
    "test_loss_MNIST, test_accuracy_MNIST = test_model(finedTunedModelMNIST, testloaderMNIST, criterion)\n",
    "print(f\"MNIST - Test Loss: {test_loss_MNIST:.4f}, Test Accuracy: {test_accuracy_MNIST:.2f}%\")\n",
    "\n",
    "print(\"Testing FashionMNIST model on clean test set...\")\n",
    "test_loss_FashionMNIST, test_accuracy_FashionMNIST = test_model(finedTunedModelFashionMNIST, testloaderFashionMNIST, criterion)\n",
    "print(f\"FashionMNIST - Test Loss: {test_loss_FashionMNIST:.4f}, Test Accuracy: {test_accuracy_FashionMNIST:.2f}%\")\n",
    "\n",
    "# Final watermark validation\n",
    "print(\"\\nFinal watermark validation...\")\n",
    "final_mnist_watermark = validate_watermark_embedding(finedTunedModelMNIST, trigger_mnist, \n",
    "                                                    torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "final_fashion_watermark = validate_watermark_embedding(finedTunedModelFashionMNIST, trigger_mnist,\n",
    "                                                      torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "print(f\"MNIST final watermark accuracy: {final_mnist_watermark:.1%}\")\n",
    "print(f\"FashionMNIST final watermark accuracy: {final_fashion_watermark:.1%}\")\n",
    "\n",
    "# Save models with proper naming\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING WATERMARKED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "\n",
    "save_watermarked_model(finedTunedModelMNIST, 'MNIST')\n",
    "save_watermarked_model(finedTunedModelFashionMNIST, 'FashionMNIST')\n",
    "\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WATERMARK EMBEDDING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MNIST Model:\")\n",
    "print(f\"  - Clean test accuracy: {test_accuracy_MNIST:.2f}%\")\n",
    "print(f\"  - Watermark accuracy: {final_mnist_watermark:.1%}\")\n",
    "print(f\"  - Embedding quality: {'✓ Strong' if final_mnist_watermark > 0.9 else '⚠️  Weak'}\")\n",
    "\n",
    "print(f\"\\nFashionMNIST Model:\")\n",
    "print(f\"  - Clean test accuracy: {test_accuracy_FashionMNIST:.2f}%\")\n",
    "print(f\"  - Watermark accuracy: {final_fashion_watermark:.1%}\")\n",
    "print(f\"  - Embedding quality: {'✓ Strong' if final_fashion_watermark > 0.9 else '⚠️  Weak'}\")\n",
    "\n",
    "print(f\"\\n✓ Watermarked models ready for attack evaluation!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
