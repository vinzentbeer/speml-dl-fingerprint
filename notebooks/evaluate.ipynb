{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed97d7d-526f-4cfe-81e7-f56eb2bd4cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images in ..\\WatermarkNN\\data\\trigger_set\\pics\n",
      "Found 100 images in ..\\WatermarkNN\\data\\trigger_set\\pics\n",
      "✓ Evaluation datasets loaded successfully\n",
      "\n",
      "=== Evaluating Original Watermarked MNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.27it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating FTLL Attacked MNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.11it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating FTAL Attacked MNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.19it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating RTLL Attacked MNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.70it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating RTAL Attacked MNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.65it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Original Watermarked FashionMNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.35it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating FTLL Attacked FashionMNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 14.10it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating FTAL Attacked FashionMNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.86it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating RTLL Attacked FashionMNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.49it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating RTAL Attacked FashionMNIST Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating clean accuracy: 100%|██████████| 157/157 [00:11<00:00, 13.46it/s]\n",
      "Evaluating watermark retention: 100%|██████████| 4/4 [00:00<00:00,  5.21it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "class TriggerDatasetPaper(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the trigger images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n",
    "        self.image_paths.extend(glob.glob(os.path.join(root_dir, '*.png'))) # Also find .png\n",
    "        print(f\"Found {len(self.image_paths)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract label from filename like \"image_0_label_7.jpeg\" -> 7\n",
    "        try:\n",
    "            filename = os.path.basename(img_path)\n",
    "            label_str = str(int(filename.split('.')[0])%10)\n",
    "            label = int(label_str)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            raise ValueError(f\"Could not parse label from filename: {img_path}. Expected format 'x.jpeg'\") from e\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def evaluate_model_accuracy(model, test_dataloader, device='cuda'):\n",
    "    \"\"\"Evaluate model accuracy on clean test data\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_dataloader, desc=\"Evaluating clean accuracy\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_watermark_retention(model, trigger_dataset, device='cuda', batch_size=32):\n",
    "    \"\"\"Evaluate how well the model retains watermark associations\"\"\"\n",
    "    model.eval()\n",
    "    trigger_loader = DataLoader(trigger_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(trigger_loader, desc=\"Evaluating watermark retention\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    watermark_accuracy = 100 * correct / total\n",
    "    \n",
    "    return {\n",
    "        'watermark_accuracy': watermark_accuracy,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'total_samples': total\n",
    "    }\n",
    "\n",
    "def comprehensive_model_evaluation(model, clean_test_loader, trigger_dataset, \n",
    "                                 model_name=\"Model\", device='cuda'):\n",
    "    \"\"\"Perform comprehensive evaluation of both primary task and watermark retention\"\"\"\n",
    "    \n",
    "    # Evaluate primary task performance\n",
    "    clean_accuracy = evaluate_model_accuracy(model, clean_test_loader, device)\n",
    "    \n",
    "    # Evaluate watermark retention\n",
    "    watermark_results = evaluate_watermark_retention(model, trigger_dataset, device)\n",
    "    \n",
    "    # Calculate attack success metrics\n",
    "    attack_success = watermark_results['watermark_accuracy'] < 50.0  # Below random chance\n",
    "    functionality_preserved = clean_accuracy > 80.0  # Maintains reasonable performance\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'clean_accuracy': clean_accuracy,\n",
    "        'watermark_accuracy': watermark_results['watermark_accuracy'],\n",
    "        'watermark_predictions': watermark_results['predictions'],\n",
    "        'watermark_labels': watermark_results['true_labels'],\n",
    "        'attack_success': attack_success,\n",
    "        'functionality_preserved': functionality_preserved,\n",
    "        'overall_attack_effectiveness': attack_success and functionality_preserved\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Setup evaluation environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transforms (matching training pipeline)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load clean test datasets\n",
    "test_mnist = MNIST(root='./data/raw/MNIST', train=False, download=True, transform=transform)\n",
    "test_fashion = FashionMNIST(root='./data/raw/FashionMNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader_mnist = DataLoader(\n",
    "    test_mnist, batch_size=100, shuffle=False, \n",
    "    num_workers=18, persistent_workers=True, pin_memory=True\n",
    ")\n",
    "test_loader_fashion = DataLoader(\n",
    "    test_fashion, batch_size=100, shuffle=False, \n",
    "    num_workers=18, persistent_workers=True, pin_memory=True\n",
    ")\n",
    "\n",
    "# Load trigger sets with updated filename parsing\n",
    "trigger_mnist = TriggerDatasetPaper(\"..\\WatermarkNN\\data\\\\trigger_set\\pics\", transform=transform)\n",
    "trigger_fashion = TriggerDatasetPaper(\"..\\WatermarkNN\\data\\\\trigger_set\\pics\", transform=transform)\n",
    "\n",
    "print(\"✓ Evaluation datasets loaded successfully\")\n",
    "\n",
    "def load_and_evaluate_models(dataset_name, clean_test_loader, trigger_dataset):\n",
    "    \"\"\"Load all model variants and perform comprehensive evaluation\"\"\"\n",
    "    \n",
    "    # Load original watermarked model\n",
    "    watermarked_model = torch.load(f'./models/watermarked_{dataset_name.lower()}_model.pth', weights_only=False)\n",
    "    watermarked_model.to(device)\n",
    "    \n",
    "    # Load all attacked models\n",
    "    attack_types = ['ftll', 'ftal', 'rtll', 'rtal']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Evaluate original watermarked model\n",
    "    print(f\"\\n=== Evaluating Original Watermarked {dataset_name} Model ===\")\n",
    "    original_results = comprehensive_model_evaluation(\n",
    "        watermarked_model, clean_test_loader, trigger_dataset, \n",
    "        f\"Original {dataset_name}\", device\n",
    "    )\n",
    "    watermarked_model.to('cpu')  # Free GPU memory\n",
    "    results.append(original_results)\n",
    "    \n",
    "    # Evaluate each attacked model\n",
    "    for attack_type in attack_types:\n",
    "        try:\n",
    "            model_path = f'./models/attacked/{dataset_name.lower()}_{attack_type}_attacked.pth'\n",
    "            attacked_model = torch.load(model_path, weights_only=False)\n",
    "            attacked_model.to(device)\n",
    "            \n",
    "            print(f\"\\n=== Evaluating {attack_type.upper()} Attacked {dataset_name} Model ===\")\n",
    "            attack_results = comprehensive_model_evaluation(\n",
    "                attacked_model, clean_test_loader, trigger_dataset,\n",
    "                f\"{attack_type.upper()} {dataset_name}\", device\n",
    "            )\n",
    "            results.append(attack_results)\n",
    "            attacked_model.to('cpu')  # Free GPU memory\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {attack_type.upper()} attacked model not found for {dataset_name}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Perform comprehensive evaluation on both datasets\n",
    "mnist_results = load_and_evaluate_models(\"MNIST\", test_loader_mnist, trigger_mnist)\n",
    "fashion_results = load_and_evaluate_models(\"FashionMNIST\", test_loader_fashion, trigger_fashion)\n",
    "\n",
    "def create_results_summary(results_list, dataset_name):\n",
    "    \"\"\"Create a comprehensive summary of evaluation results\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    for result in results_list:\n",
    "        summary_data.append({\n",
    "            'Model': result['model_name'],\n",
    "            'Clean Accuracy (%)': f\"{result['clean_accuracy']:.2f}\",\n",
    "            'Watermark Accuracy (%)': f\"{result['watermark_accuracy']:.2f}\",\n",
    "            'Attack Success': \"✓\" if result['attack_success'] else \"✗\",\n",
    "            'Functionality Preserved': \"✓\" if result['functionality_preserved'] else \"✗\",\n",
    "            'Overall Effectiveness': \"✓\" if result['overall_attack_effectiveness'] else \"✗\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPREHENSIVE EVALUATION SUMMARY - {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate comprehensive summaries\n",
    "mnist_summary = create_results_summary(mnist_results, \"MNIST\")\n",
    "fashion_summary = create_results_summary(fashion_results, \"FashionMNIST\")\n",
    "\n",
    "def analyze_attack_effectiveness(results_list, dataset_name):\n",
    "    \"\"\"Analyze and visualize attack effectiveness patterns\"\"\"\n",
    "    \n",
    "    model_names = [r['model_name'] for r in results_list]\n",
    "    clean_accuracies = [r['clean_accuracy'] for r in results_list]\n",
    "    watermark_accuracies = [r['watermark_accuracy'] for r in results_list]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Clean accuracy comparison\n",
    "    ax1.bar(range(len(model_names)), clean_accuracies, color='blue', alpha=0.7)\n",
    "    ax1.set_xlabel('Model Variants')\n",
    "    ax1.set_ylabel('Clean Accuracy (%)')\n",
    "    ax1.set_title(f'{dataset_name} - Primary Task Performance')\n",
    "    ax1.set_xticks(range(len(model_names)))\n",
    "    ax1.set_xticklabels([name.split()[-1] for name in model_names], rotation=45)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for i, v in enumerate(clean_accuracies):\n",
    "        ax1.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # Watermark retention comparison\n",
    "    ax2.bar(range(len(model_names)), watermark_accuracies, color='red', alpha=0.7)\n",
    "    ax2.set_xlabel('Model Variants')\n",
    "    ax2.set_ylabel('Watermark Accuracy (%)')\n",
    "    ax2.set_title(f'{dataset_name} - Watermark Retention')\n",
    "    ax2.set_xticks(range(len(model_names)))\n",
    "    ax2.set_xticklabels([name.split()[-1] for name in model_names], rotation=45)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.axhline(y=50, color='green', linestyle='--', alpha=0.7, label='Random Chance')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for i, v in enumerate(watermark_accuracies):\n",
    "        ax2.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./results/{dataset_name.lower()}_attack_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    original_watermark_acc = results_list[0]['watermark_accuracy']\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Attack Effectiveness Analysis:\")\n",
    "    print(f\"Original watermark strength: {original_watermark_acc:.2f}%\")\n",
    "    \n",
    "    for i, result in enumerate(results_list[1:], 1):\n",
    "        attack_name = result['model_name'].split()[0]\n",
    "        watermark_reduction = original_watermark_acc - result['watermark_accuracy']\n",
    "        clean_preservation = result['clean_accuracy']\n",
    "        \n",
    "        print(f\"{attack_name}: -{watermark_reduction:.2f}% watermark retention, \"\n",
    "              f\"{clean_preservation:.2f}% clean accuracy preserved\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "# Perform detailed analysis\n",
    "analyze_attack_effectiveness(mnist_results, \"MNIST\")\n",
    "analyze_attack_effectiveness(fashion_results, \"FashionMNIST\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WATERMARK ROBUSTNESS EVALUATION COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Primary task performance measured for all model variants\")\n",
    "print(\"✓ Watermark retention assessed across all attack types\")\n",
    "print(\"✓ Attack effectiveness quantified and visualized\")\n",
    "print(\"✓ Results saved to ./results/ directory\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
