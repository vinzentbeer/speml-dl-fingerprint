{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32b9c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18 # <-- IMPORTANT: Replace with your model import\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import torchvision\n",
    "\n",
    "# --- 1. Custom Dataset for the Trigger Set ---\n",
    "# This dataset loads images from a folder where filenames are like \"XX_label.jpeg\"\n",
    "class TriggerSetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the trigger images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n",
    "        self.image_paths.extend(glob.glob(os.path.join(root_dir, '*.png'))) # Also find .png\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract label from filename like \"image_0_label_7.jpeg\" -> 7\n",
    "        try:\n",
    "            filename = os.path.basename(img_path)\n",
    "            label_str = filename.split('_')[-1].split('.')[0]\n",
    "            label = int(label_str)\n",
    "        except (IndexError, ValueError) as e:\n",
    "            raise ValueError(f\"Could not parse label from filename: {img_path}. Expected format '..._label.ext'\") from e\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# --- 2. Helper Function to Load Your Model ---\n",
    "def load_full_model(model_path):\n",
    "    \"\"\"Loads a full model object that was saved with torch.save(model, path).\"\"\"\n",
    "    print(f\"Loading full model from {model_path}\")\n",
    "    \n",
    "    # Loading is a single step. No need to instantiate the class first.\n",
    "    # Use map_location for portability (e.g., loading a GPU-trained model on a CPU)\n",
    "    with torch.serialization.safe_globals([\n",
    "    torchvision.models.squeezenet.SqueezeNet,\n",
    "    torch.nn.modules.container.Sequential,\n",
    "]):\n",
    "        model = torch.load(model_path, map_location=torch.device('cpu'),weights_only=False)\n",
    "    \n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "def get_squeezenet_last_layer(model):\n",
    "    \"\"\"Returns the last trainable layer of a SqueezeNet model.\"\"\"\n",
    "    # The final classification layer in SqueezeNet is a Conv2d layer\n",
    "    # located at index 1 of the 'classifier' sequential module.\n",
    "    return model.classifier[1]\n",
    "\n",
    "# --- 3. A Generic Training Loop ---\n",
    "def run_training(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        if epoch%10 == 0:\n",
    "            #reduce learning rate by factor of 10 every 10 epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- 4. The Four Fine-Tuning Functions ---\n",
    "def fine_tune_last_layer(model, trigger_set_path, lr=0.1, num_epochs=10, batch_size=4):\n",
    "    \"\"\" Fine-Tune Last Layer (FTLL): Freeze all layers except the last one and train. \"\"\"\n",
    "    print(\"\\n--- Starting: Fine-Tune Last Layer (FTLL) ---\")\n",
    "    \n",
    "    # Freeze all parameters in the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze the parameters of the last layer (e.g., the fully connected layer in ResNet)\n",
    "    # IMPORTANT: You must change 'fc' to the name of your model's last layer.\n",
    "    last_layer = get_squeezenet_last_layer(model)\n",
    "\n",
    "    for param in last_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # Create an optimizer that only updates the unfrozen (trainable) parameters\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Setup data, criterion, and run training\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # convert 1->3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    dataset = TriggerSetDataset(root_dir=trigger_set_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    trained_model = run_training(model, dataloader, criterion, optimizer, device, num_epochs)\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def fine_tune_all_layers(model, trigger_set_path, lr=0.1, num_epochs=10, batch_size=4):\n",
    "    \"\"\" Fine-Tune All Layers (FTAL): Unfreeze all layers and train. \"\"\"\n",
    "    print(\"\\n--- Starting: Fine-Tune All Layers (FTAL) ---\")\n",
    "    \n",
    "    # Ensure all parameters are trainable (this is the default state)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"All layers are unfrozen for training.\")\n",
    "\n",
    "    # Optimizer for all parameters\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Setup data, criterion, and run training\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # convert 1->3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    dataset = TriggerSetDataset(root_dir=trigger_set_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    trained_model = run_training(model, dataloader, criterion, optimizer, device, num_epochs)\n",
    "    return trained_model\n",
    "\n",
    "# Helper function to re-initialize weights\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def retrain_last_layer(model, trigger_set_path, lr=0.1, num_epochs=10, batch_size=4):\n",
    "    \"\"\" Retrain Last Layer (RTLL): Re-initialize last layer, then freeze all others and train it. \"\"\"\n",
    "    print(\"\\n--- Starting: Retrain Last Layer (RTLL) ---\")\n",
    "    \n",
    "    # Freeze all parameters in the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Re-initialize the weights of the last layer\n",
    "    # IMPORTANT: You must change 'fc' to the name of your model's last layer.\n",
    "\n",
    "    last_layer = get_squeezenet_last_layer(model)\n",
    "    last_layer.apply(weight_reset)\n",
    "    # Unfreeze the parameters of the last layer so it can be trained\n",
    "   \n",
    "    for param in last_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Setup data, criterion, and run training\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # convert 1->3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    dataset = TriggerSetDataset(root_dir=trigger_set_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    trained_model = run_training(model, dataloader, criterion, optimizer, device, num_epochs)\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def retrain_all_layers(model, trigger_set_path, lr=0.1, num_epochs=10, batch_size=4):\n",
    "    \"\"\" Retrain All Layers (RTAL): Re-initialize the entire model and train from scratch. \"\"\"\n",
    "    print(\"\\n--- Starting: Retrain All Layers (RTAL) ---\")\n",
    "    \n",
    "    # Re-initialize all weights in the model\n",
    "    print(\"Re-initializing all weights in the model.\")\n",
    "    model.apply(weight_reset)\n",
    "    \n",
    "    # Ensure all parameters are trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Setup data, criterion, and run training\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # convert 1->3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    dataset = TriggerSetDataset(root_dir=trigger_set_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    trained_model = run_training(model, dataloader, criterion, optimizer, device, num_epochs)\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbb52026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model: MNIST from notebooks\\models\\MNIST_SN_finetuned_baseline.pth\n",
      "\n",
      "Processing trigger set: triggerset1 at data\\trigger_sets\\triggerset1\n",
      "Loading full model from notebooks\\models\\MNIST_SN_finetuned_baseline.pth\n",
      "Model loaded successfully.\n",
      "Applying fine-tuning method: FTLL\n",
      "\n",
      "--- Starting: Fine-Tune Last Layer (FTLL) ---\n",
      "Epoch [1/30], Loss: 7.5338\n",
      "Epoch [2/30], Loss: 3.3473\n",
      "Epoch [3/30], Loss: 2.6752\n",
      "Epoch [4/30], Loss: 2.5917\n",
      "Epoch [5/30], Loss: 2.4489\n",
      "Epoch [6/30], Loss: 2.3838\n",
      "Epoch [7/30], Loss: 2.3360\n",
      "Epoch [8/30], Loss: 2.2682\n",
      "Epoch [9/30], Loss: 2.2368\n",
      "Epoch [10/30], Loss: 2.2307\n",
      "Epoch [11/30], Loss: 2.1757\n",
      "Epoch [12/30], Loss: 2.2053\n",
      "Epoch [13/30], Loss: 2.2172\n",
      "Epoch [14/30], Loss: 2.1765\n",
      "Epoch [15/30], Loss: 2.1363\n",
      "Epoch [16/30], Loss: 2.1748\n",
      "Epoch [17/30], Loss: 2.1650\n",
      "Epoch [18/30], Loss: 2.1529\n",
      "Epoch [19/30], Loss: 2.1972\n",
      "Epoch [20/30], Loss: 2.1127\n",
      "Epoch [21/30], Loss: 2.1089\n",
      "Epoch [22/30], Loss: 2.1025\n",
      "Epoch [23/30], Loss: 2.1772\n",
      "Epoch [24/30], Loss: 2.1736\n",
      "Epoch [25/30], Loss: 2.2359\n",
      "Epoch [26/30], Loss: 2.1711\n",
      "Epoch [27/30], Loss: 2.1127\n",
      "Epoch [28/30], Loss: 2.1429\n",
      "Epoch [29/30], Loss: 2.1302\n",
      "Epoch [30/30], Loss: 2.1487\n",
      "Model saved to notebooks/models/MNIST_triggerset1_FTLL.pth\n",
      "Applying fine-tuning method: FTAL\n",
      "\n",
      "--- Starting: Fine-Tune All Layers (FTAL) ---\n",
      "All layers are unfrozen for training.\n",
      "Epoch [1/30], Loss: 4.6458\n",
      "Epoch [2/30], Loss: 2.4675\n",
      "Epoch [3/30], Loss: 2.2870\n",
      "Epoch [4/30], Loss: 2.2579\n",
      "Epoch [5/30], Loss: 2.2261\n",
      "Epoch [6/30], Loss: 2.1532\n",
      "Epoch [7/30], Loss: 2.1790\n",
      "Epoch [8/30], Loss: 2.0745\n",
      "Epoch [9/30], Loss: 2.0269\n",
      "Epoch [10/30], Loss: 2.0878\n",
      "Epoch [11/30], Loss: 1.9974\n",
      "Epoch [12/30], Loss: 2.0236\n",
      "Epoch [13/30], Loss: 2.0367\n",
      "Epoch [14/30], Loss: 2.0564\n",
      "Epoch [15/30], Loss: 2.0411\n",
      "Epoch [16/30], Loss: 2.0176\n",
      "Epoch [17/30], Loss: 2.0219\n",
      "Epoch [18/30], Loss: 2.0008\n",
      "Epoch [19/30], Loss: 2.0165\n",
      "Epoch [20/30], Loss: 1.9690\n",
      "Epoch [21/30], Loss: 1.9709\n",
      "Epoch [22/30], Loss: 2.0307\n",
      "Epoch [23/30], Loss: 1.9863\n",
      "Epoch [24/30], Loss: 2.0136\n",
      "Epoch [25/30], Loss: 2.0123\n",
      "Epoch [26/30], Loss: 1.9875\n",
      "Epoch [27/30], Loss: 1.9828\n",
      "Epoch [28/30], Loss: 2.0027\n",
      "Epoch [29/30], Loss: 1.9656\n",
      "Epoch [30/30], Loss: 2.0104\n",
      "Model saved to notebooks/models/MNIST_triggerset1_FTAL.pth\n",
      "Applying fine-tuning method: RTLL\n",
      "\n",
      "--- Starting: Retrain Last Layer (RTLL) ---\n",
      "Epoch [1/30], Loss: 4.1826\n",
      "Epoch [2/30], Loss: 2.5131\n",
      "Epoch [3/30], Loss: 2.3140\n",
      "Epoch [4/30], Loss: 2.2519\n",
      "Epoch [5/30], Loss: 2.2545\n",
      "Epoch [6/30], Loss: 2.1933\n",
      "Epoch [7/30], Loss: 2.2373\n",
      "Epoch [8/30], Loss: 2.1918\n",
      "Epoch [9/30], Loss: 2.2063\n",
      "Epoch [10/30], Loss: 2.1531\n",
      "Epoch [11/30], Loss: 2.1067\n",
      "Epoch [12/30], Loss: 2.1104\n",
      "Epoch [13/30], Loss: 2.1006\n",
      "Epoch [14/30], Loss: 2.0839\n",
      "Epoch [15/30], Loss: 2.1039\n",
      "Epoch [16/30], Loss: 2.1129\n",
      "Epoch [17/30], Loss: 2.0818\n",
      "Epoch [18/30], Loss: 2.1014\n",
      "Epoch [19/30], Loss: 2.1137\n",
      "Epoch [20/30], Loss: 2.0875\n",
      "Epoch [21/30], Loss: 2.0429\n",
      "Epoch [22/30], Loss: 2.0803\n",
      "Epoch [23/30], Loss: 2.0869\n",
      "Epoch [24/30], Loss: 2.1059\n",
      "Epoch [25/30], Loss: 2.0699\n",
      "Epoch [26/30], Loss: 2.0913\n",
      "Epoch [27/30], Loss: 2.0892\n",
      "Epoch [28/30], Loss: 2.1017\n",
      "Epoch [29/30], Loss: 2.0763\n",
      "Epoch [30/30], Loss: 2.0742\n",
      "Model saved to notebooks/models/MNIST_triggerset1_RTLL.pth\n",
      "Applying fine-tuning method: RTAL\n",
      "\n",
      "--- Starting: Retrain All Layers (RTAL) ---\n",
      "Re-initializing all weights in the model.\n",
      "Epoch [1/30], Loss: 2.3038\n",
      "Epoch [2/30], Loss: 2.3038\n",
      "Epoch [3/30], Loss: 2.3036\n",
      "Epoch [4/30], Loss: 2.3036\n",
      "Epoch [5/30], Loss: 2.3036\n",
      "Epoch [6/30], Loss: 2.3034\n",
      "Epoch [7/30], Loss: 2.3034\n",
      "Epoch [8/30], Loss: 2.3033\n",
      "Epoch [9/30], Loss: 2.3032\n",
      "Epoch [10/30], Loss: 2.3031\n",
      "Epoch [11/30], Loss: 2.3031\n",
      "Epoch [12/30], Loss: 2.3030\n",
      "Epoch [13/30], Loss: 2.3031\n",
      "Epoch [14/30], Loss: 2.3029\n",
      "Epoch [15/30], Loss: 2.3031\n",
      "Epoch [16/30], Loss: 2.3030\n",
      "Epoch [17/30], Loss: 2.3029\n",
      "Epoch [18/30], Loss: 2.3029\n",
      "Epoch [19/30], Loss: 2.3031\n",
      "Epoch [20/30], Loss: 2.3029\n",
      "Epoch [21/30], Loss: 2.3031\n",
      "Epoch [22/30], Loss: 2.3029\n",
      "Epoch [23/30], Loss: 2.3030\n",
      "Epoch [24/30], Loss: 2.3029\n",
      "Epoch [25/30], Loss: 2.3030\n",
      "Epoch [26/30], Loss: 2.3028\n",
      "Epoch [27/30], Loss: 2.3028\n",
      "Epoch [28/30], Loss: 2.3029\n",
      "Epoch [29/30], Loss: 2.3029\n",
      "Epoch [30/30], Loss: 2.3029\n",
      "Model saved to notebooks/models/MNIST_triggerset1_RTAL.pth\n",
      "\n",
      "Processing model: FMNIST from notebooks\\models\\FMNIST_SN_finetuned_baseline.pth\n",
      "\n",
      "Processing trigger set: triggerset1 at data\\trigger_sets\\triggerset1\n",
      "Loading full model from notebooks\\models\\FMNIST_SN_finetuned_baseline.pth\n",
      "Model loaded successfully.\n",
      "Applying fine-tuning method: FTLL\n",
      "\n",
      "--- Starting: Fine-Tune Last Layer (FTLL) ---\n",
      "Epoch [1/30], Loss: 3.8415\n",
      "Epoch [2/30], Loss: 3.1070\n",
      "Epoch [3/30], Loss: 2.9398\n",
      "Epoch [4/30], Loss: 2.7136\n",
      "Epoch [5/30], Loss: 2.6311\n",
      "Epoch [6/30], Loss: 2.5036\n",
      "Epoch [7/30], Loss: 2.3538\n",
      "Epoch [8/30], Loss: 2.3270\n",
      "Epoch [9/30], Loss: 2.3255\n",
      "Epoch [10/30], Loss: 2.2760\n",
      "Epoch [11/30], Loss: 2.2420\n",
      "Epoch [12/30], Loss: 2.2344\n",
      "Epoch [13/30], Loss: 2.1945\n",
      "Epoch [14/30], Loss: 2.2349\n",
      "Epoch [15/30], Loss: 2.2739\n",
      "Epoch [16/30], Loss: 2.1687\n",
      "Epoch [17/30], Loss: 2.2082\n",
      "Epoch [18/30], Loss: 2.1364\n",
      "Epoch [19/30], Loss: 2.2112\n",
      "Epoch [20/30], Loss: 2.2749\n",
      "Epoch [21/30], Loss: 2.2092\n",
      "Epoch [22/30], Loss: 2.1916\n",
      "Epoch [23/30], Loss: 2.2004\n",
      "Epoch [24/30], Loss: 2.1771\n",
      "Epoch [25/30], Loss: 2.1768\n",
      "Epoch [26/30], Loss: 2.2567\n",
      "Epoch [27/30], Loss: 2.2122\n",
      "Epoch [28/30], Loss: 2.2301\n",
      "Epoch [29/30], Loss: 2.2458\n",
      "Epoch [30/30], Loss: 2.2329\n",
      "Model saved to notebooks/models/FMNIST_triggerset1_FTLL.pth\n",
      "Applying fine-tuning method: FTAL\n",
      "\n",
      "--- Starting: Fine-Tune All Layers (FTAL) ---\n",
      "All layers are unfrozen for training.\n",
      "Epoch [1/30], Loss: 3.5168\n",
      "Epoch [2/30], Loss: 2.5953\n",
      "Epoch [3/30], Loss: 2.4078\n",
      "Epoch [4/30], Loss: 2.3285\n",
      "Epoch [5/30], Loss: 2.2850\n",
      "Epoch [6/30], Loss: 2.1962\n",
      "Epoch [7/30], Loss: 2.1709\n",
      "Epoch [8/30], Loss: 2.1281\n",
      "Epoch [9/30], Loss: 2.1312\n",
      "Epoch [10/30], Loss: 2.1024\n",
      "Epoch [11/30], Loss: 2.0370\n",
      "Epoch [12/30], Loss: 2.0439\n",
      "Epoch [13/30], Loss: 2.0355\n",
      "Epoch [14/30], Loss: 2.0619\n",
      "Epoch [15/30], Loss: 2.0605\n",
      "Epoch [16/30], Loss: 2.0554\n",
      "Epoch [17/30], Loss: 2.0528\n",
      "Epoch [18/30], Loss: 2.0061\n",
      "Epoch [19/30], Loss: 2.0373\n",
      "Epoch [20/30], Loss: 2.0420\n",
      "Epoch [21/30], Loss: 2.0578\n",
      "Epoch [22/30], Loss: 1.9807\n",
      "Epoch [23/30], Loss: 1.9986\n",
      "Epoch [24/30], Loss: 1.9783\n",
      "Epoch [25/30], Loss: 1.9728\n",
      "Epoch [26/30], Loss: 2.0159\n",
      "Epoch [27/30], Loss: 2.0145\n",
      "Epoch [28/30], Loss: 2.0087\n",
      "Epoch [29/30], Loss: 2.0199\n",
      "Epoch [30/30], Loss: 2.0067\n",
      "Model saved to notebooks/models/FMNIST_triggerset1_FTAL.pth\n",
      "Applying fine-tuning method: RTLL\n",
      "\n",
      "--- Starting: Retrain Last Layer (RTLL) ---\n",
      "Epoch [1/30], Loss: 2.3995\n",
      "Epoch [2/30], Loss: 2.2526\n",
      "Epoch [3/30], Loss: 2.2478\n",
      "Epoch [4/30], Loss: 2.2024\n",
      "Epoch [5/30], Loss: 2.1779\n",
      "Epoch [6/30], Loss: 2.1343\n",
      "Epoch [7/30], Loss: 2.0940\n",
      "Epoch [8/30], Loss: 2.0592\n",
      "Epoch [9/30], Loss: 2.0994\n",
      "Epoch [10/30], Loss: 2.0483\n",
      "Epoch [11/30], Loss: 2.0150\n",
      "Epoch [12/30], Loss: 2.0197\n",
      "Epoch [13/30], Loss: 2.0197\n",
      "Epoch [14/30], Loss: 1.9751\n",
      "Epoch [15/30], Loss: 2.0010\n",
      "Epoch [16/30], Loss: 1.9724\n",
      "Epoch [17/30], Loss: 1.9962\n",
      "Epoch [18/30], Loss: 2.0118\n",
      "Epoch [19/30], Loss: 1.9887\n",
      "Epoch [20/30], Loss: 2.0007\n",
      "Epoch [21/30], Loss: 1.9792\n",
      "Epoch [22/30], Loss: 1.9934\n",
      "Epoch [23/30], Loss: 1.9583\n",
      "Epoch [24/30], Loss: 2.0150\n",
      "Epoch [25/30], Loss: 1.9819\n",
      "Epoch [26/30], Loss: 1.9739\n",
      "Epoch [27/30], Loss: 1.9955\n",
      "Epoch [28/30], Loss: 1.9789\n",
      "Epoch [29/30], Loss: 1.9972\n",
      "Epoch [30/30], Loss: 1.9672\n",
      "Model saved to notebooks/models/FMNIST_triggerset1_RTLL.pth\n",
      "Applying fine-tuning method: RTAL\n",
      "\n",
      "--- Starting: Retrain All Layers (RTAL) ---\n",
      "Re-initializing all weights in the model.\n",
      "Epoch [1/30], Loss: 2.3037\n",
      "Epoch [2/30], Loss: 2.3036\n",
      "Epoch [3/30], Loss: 2.3036\n",
      "Epoch [4/30], Loss: 2.3034\n",
      "Epoch [5/30], Loss: 2.3032\n",
      "Epoch [6/30], Loss: 2.3032\n",
      "Epoch [7/30], Loss: 2.3032\n",
      "Epoch [8/30], Loss: 2.3028\n",
      "Epoch [9/30], Loss: 2.3028\n",
      "Epoch [10/30], Loss: 2.3028\n",
      "Epoch [11/30], Loss: 2.3026\n",
      "Epoch [12/30], Loss: 2.3026\n",
      "Epoch [13/30], Loss: 2.3026\n",
      "Epoch [14/30], Loss: 2.3025\n",
      "Epoch [15/30], Loss: 2.3025\n",
      "Epoch [16/30], Loss: 2.3025\n",
      "Epoch [17/30], Loss: 2.3027\n",
      "Epoch [18/30], Loss: 2.3026\n",
      "Epoch [19/30], Loss: 2.3027\n",
      "Epoch [20/30], Loss: 2.3024\n",
      "Epoch [21/30], Loss: 2.3023\n",
      "Epoch [22/30], Loss: 2.3026\n",
      "Epoch [23/30], Loss: 2.3026\n",
      "Epoch [24/30], Loss: 2.3024\n",
      "Epoch [25/30], Loss: 2.3024\n",
      "Epoch [26/30], Loss: 2.3025\n",
      "Epoch [27/30], Loss: 2.3025\n",
      "Epoch [28/30], Loss: 2.3026\n",
      "Epoch [29/30], Loss: 2.3025\n",
      "Epoch [30/30], Loss: 2.3025\n",
      "Model saved to notebooks/models/FMNIST_triggerset1_RTAL.pth\n"
     ]
    }
   ],
   "source": [
    "# path to triggersets ./../data/trigger_sets/ 10 folders inside\n",
    "BASE_TRIGGER_SET_PATH = os.path.join( 'data', 'trigger_sets')\n",
    "baseline_MNIST_model_path = os.path.join('notebooks','models', 'MNIST_SN_finetuned_baseline.pth')\n",
    "baseline_FMNIST_model_path = os.path.join('notebooks','models', 'FMNIST_SN_finetuned_baseline.pth')\n",
    "\n",
    "BASELINE_MODELS = {\n",
    "        'MNIST': baseline_MNIST_model_path,\n",
    "        'FMNIST': baseline_FMNIST_model_path,\n",
    "    }\n",
    "    \n",
    "FINETUNING_METHODS = {\n",
    "    'FTLL': fine_tune_last_layer,\n",
    "    'FTAL': fine_tune_all_layers,\n",
    "    'RTLL': retrain_last_layer,\n",
    "    'RTAL': retrain_all_layers,\n",
    "}\n",
    "\n",
    "trigger_folders = [f for f in os.listdir(BASE_TRIGGER_SET_PATH) if os.path.isdir(os.path.join(BASE_TRIGGER_SET_PATH, f))]\n",
    "\n",
    "for modelname,modelpath in BASELINE_MODELS.items():\n",
    "    print(f\"\\nProcessing model: {modelname} from {modelpath}\")\n",
    "    \n",
    "    # Load the model\n",
    "    \n",
    "    \n",
    "    for trigger_folder in trigger_folders[0:1]:  # Change slice to process all folders or specific ones\n",
    "        trigger_set_path = os.path.join(BASE_TRIGGER_SET_PATH, trigger_folder)\n",
    "        print(f\"\\nProcessing trigger set: {trigger_folder} at {trigger_set_path}\")\n",
    "        \n",
    "        model = load_full_model(modelpath)\n",
    "        \n",
    "        for method_name, method_func in FINETUNING_METHODS.items():\n",
    "            print(f\"Applying fine-tuning method: {method_name}\")\n",
    "            try:\n",
    "                trained_model = method_func(copy.deepcopy(model), trigger_set_path,num_epochs=30,lr=0.001)\n",
    "                # Save the trained model if needed\n",
    "                save_path = f\"notebooks/models/{modelname}_{trigger_folder}_{method_name}.pth\"\n",
    "                torch.save(trained_model, save_path)\n",
    "                print(f\"Model saved to {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during {method_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0c6772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trigger set data from: data\\trigger_sets\\triggerset1\n",
      "\n",
      "Found 10 models. Starting evaluation...\n",
      "--- Testing Model: FMNIST_SN_finetuned_baseline.pth ---\n",
      "Loading full model from notebooks/models\\FMNIST_SN_finetuned_baseline.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 6.00% (6/100 correct)\n",
      "--- Testing Model: FMNIST_triggerset1_FTAL.pth ---\n",
      "Loading full model from notebooks/models\\FMNIST_triggerset1_FTAL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 33.00% (33/100 correct)\n",
      "--- Testing Model: FMNIST_triggerset1_FTLL.pth ---\n",
      "Loading full model from notebooks/models\\FMNIST_triggerset1_FTLL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 23.00% (23/100 correct)\n",
      "--- Testing Model: FMNIST_triggerset1_RTAL.pth ---\n",
      "Loading full model from notebooks/models\\FMNIST_triggerset1_RTAL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 12.00% (12/100 correct)\n",
      "--- Testing Model: FMNIST_triggerset1_RTLL.pth ---\n",
      "Loading full model from notebooks/models\\FMNIST_triggerset1_RTLL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 33.00% (33/100 correct)\n",
      "--- Testing Model: MNIST_SN_finetuned_baseline.pth ---\n",
      "Loading full model from notebooks/models\\MNIST_SN_finetuned_baseline.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 7.00% (7/100 correct)\n",
      "--- Testing Model: MNIST_triggerset1_FTAL.pth ---\n",
      "Loading full model from notebooks/models\\MNIST_triggerset1_FTAL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 33.00% (33/100 correct)\n",
      "--- Testing Model: MNIST_triggerset1_FTLL.pth ---\n",
      "Loading full model from notebooks/models\\MNIST_triggerset1_FTLL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 21.00% (21/100 correct)\n",
      "--- Testing Model: MNIST_triggerset1_RTAL.pth ---\n",
      "Loading full model from notebooks/models\\MNIST_triggerset1_RTAL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 10.00% (10/100 correct)\n",
      "--- Testing Model: MNIST_triggerset1_RTLL.pth ---\n",
      "Loading full model from notebooks/models\\MNIST_triggerset1_RTLL.pth\n",
      "Model loaded successfully.\n",
      "  > Accuracy: 26.00% (26/100 correct)\n"
     ]
    }
   ],
   "source": [
    "# load all models inside the models folder one by one\n",
    "# test the triggerset1 on it and print the accuracy\n",
    "TRANSFORM_SQUARE = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # convert 1->3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def evaluate_models_on_triggerset(trigger_set_path, model_dir='notebooks/models'):\n",
    "    \"\"\"Load all models from a directory and test their accuracy on a single trigger set.\"\"\"\n",
    "    \n",
    "    # --- 1. Load the Data ONCE ---\n",
    "    print(f\"Loading trigger set data from: {trigger_set_path}\")\n",
    "    try:\n",
    "        dataset = TriggerSetDataset(root_dir=trigger_set_path, transform=TRANSFORM_SQUARE)\n",
    "        # FIX #1: Check if the dataset is empty right away.\n",
    "        if not dataset:\n",
    "            print(f\"CRITICAL ERROR: No data loaded from {trigger_set_path}. Please check the path and file contents.\")\n",
    "            return # Exit the function\n",
    "        dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Failed to load dataset. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Find and Loop Through Models ---\n",
    "    model_files = glob.glob(os.path.join(model_dir, '*.pth'))\n",
    "    if not model_files:\n",
    "        print(f\"No models found in '{model_dir}'.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nFound {len(model_files)} models. Starting evaluation...\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            print(f\"--- Testing Model: {os.path.basename(model_file)} ---\")\n",
    "            model = load_full_model(model_file)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in dataloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # FIX #2: Add a safety check before division to prevent the crash.\n",
    "            if total > 0:\n",
    "                accuracy = (correct / total) * 100\n",
    "                print(f\"  > Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "            else:\n",
    "                # This case should not be reached if the early exit works, but it's good practice.\n",
    "                print(\"  > Accuracy: N/A (No data was processed)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  > An unexpected error occurred while testing {os.path.basename(model_file)}: {e}\")\n",
    "            \n",
    "BASE_TRIGGER_SET_PATH = os.path.join('data', 'trigger_sets')\n",
    "MODEL_DIRECTORY = 'notebooks/models'\n",
    "target_trigger_set_path = os.path.join(BASE_TRIGGER_SET_PATH, 'triggerset1')\n",
    "    \n",
    "evaluate_models_on_triggerset(target_trigger_set_path, model_dir=MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ac25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14271e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
