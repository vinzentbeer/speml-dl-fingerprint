{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd76a6b9-5150-447b-8d86-842dbd63aa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded pre-trained watermarked models for attack evaluation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ===== CHANGED: Load pre-trained watermarked models instead of clean models =====\n",
    "# Previously: Started with clean baseline models\n",
    "# Now: Load models that already have watermarks embedded during training\n",
    "\n",
    "def load_watermarked_models():\n",
    "    \"\"\"Load pre-trained watermarked models from the training phase\"\"\"\n",
    "    # These should be the models saved after watermark embedding in notebook 02_\n",
    "    watermarked_model_mnist = torch.load('./models/MNIST_SN_finetuned_baseline.pth')\n",
    "    watermarked_model_fashion = torch.load('./models/FMNIST_SN_finetuned_baseline.pth')\n",
    "    \n",
    "    return watermarked_model_mnist, watermarked_model_fashion\n",
    "\n",
    "# Load the watermarked models\n",
    "watermarked_modelMNIST, watermarked_modelFashionMNIST = load_watermarked_models()\n",
    "\n",
    "print(\"✓ Loaded pre-trained watermarked models for attack evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7aa382-85fe-41a7-bd99-edf275f11765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prepared clean datasets for watermark removal attacks\n"
     ]
    }
   ],
   "source": [
    "# ===== CHANGED: Use clean datasets for attacks, not trigger sets =====\n",
    "# Previously: Used trigger sets for fine-tuning\n",
    "# Now: Use original clean datasets to simulate realistic attack scenarios\n",
    "\n",
    "# Define transforms (same as training, but without watermark integration)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Force exact dimensions\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CLEAN datasets for attacks (no watermark integration)\n",
    "clean_dsMNIST = MNIST(root='./data/raw/MNIST', train=True, download=True, transform=transform)\n",
    "clean_dsFashionMNIST = FashionMNIST(root='./data/raw/FashionMNIST', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create clean dataloaders for attacks\n",
    "bsize = 64\n",
    "clean_trainloaderMNIST = DataLoader(clean_dsMNIST, batch_size=bsize, shuffle=True)\n",
    "clean_trainloaderFashionMNIST = DataLoader(clean_dsFashionMNIST, batch_size=bsize, shuffle=True)\n",
    "\n",
    "print(\"✓ Prepared clean datasets for watermark removal attacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f2a2bd-305e-439e-84a4-2194ca0cbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attack_ftll(watermarked_model, clean_dataloader, num_epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Fine-Tune Last Layer (FTLL) Attack\n",
    "    Attempts to remove watermarks by only modifying the final classification layer\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(watermarked_model)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # ===== CHANGED: Freeze feature layers, only train classifier =====\n",
    "    # Previously: Trained all layers during FTLL\n",
    "    # Now: Properly isolate final layer training\n",
    "    \n",
    "    # Freeze all feature extraction layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Enable training only for the final classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # ===== CHANGED: Higher learning rate for effective watermark removal =====\n",
    "    # Previously: lr=0.001 (too conservative)\n",
    "    # Now: lr=0.01 (aggressive enough to overwrite watermark patterns)\n",
    "    \n",
    "    optimizer = optim.SGD(model.classifier.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(clean_dataloader, desc=f\"FTLL Attack Epoch {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def attack_ftal(watermarked_model, clean_dataloader, num_epochs=15, lr=0.01):\n",
    "    \"\"\"\n",
    "    Fine-Tune All Layers (FTAL) Attack\n",
    "    Most aggressive attack - attempts to overwrite all watermark patterns\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(watermarked_model)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # ===== CHANGED: Enable training for all parameters =====\n",
    "    # Previously: Inconsistent parameter training\n",
    "    # Now: Full model retraining with clean data\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # ===== CHANGED: Higher learning rate with decay schedule =====\n",
    "    # Previously: Fixed low learning rate\n",
    "    # Now: Aggressive initial rate with strategic decay\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(clean_dataloader, desc=f\"FTAL Attack Epoch {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def attack_rtll(watermarked_model, clean_dataloader, num_epochs=10, lr=0.01):\n",
    "    \"\"\"\n",
    "    Retrain Last Layer (RTLL) Attack\n",
    "    Reinitializes the final layer before training - more aggressive than FTLL\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(watermarked_model)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # ===== CHANGED: Proper layer reinitialization =====\n",
    "    # Previously: Didn't actually reinitialize layers\n",
    "    # Now: Reset final layer weights before training\n",
    "    \n",
    "    # Freeze feature layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Reinitialize the final classifier layer\n",
    "    if hasattr(model, 'classifier'):\n",
    "        for layer in model.classifier:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    # Enable training only for classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.SGD(model.classifier.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(clean_dataloader, desc=f\"RTLL Attack Epoch {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def attack_rtal(watermarked_model, clean_dataloader, num_epochs=20, lr=0.01):\n",
    "    \"\"\"\n",
    "    Retrain All Layers (RTAL) Attack\n",
    "    Complete model reinitialization and retraining - most aggressive attack\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(watermarked_model)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # ===== CHANGED: Complete model reinitialization =====\n",
    "    # Previously: Partial reinitialization\n",
    "    # Now: Reset all trainable parameters\n",
    "    \n",
    "    # Reinitialize all layers\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, 'weight'):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    # Enable training for all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(clean_dataloader, desc=f\"RTAL Attack Epoch {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556cfe7f-b09e-4ceb-bc55-071a17aafe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Executing attacks on MNIST watermarked model ===\n",
      "\n",
      "1. Fine-Tune Last Layer (FTLL) Attack\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FTLL Attack Epoch 1/10: 100%|██████████| 938/938 [01:42<00:00,  9.14it/s, acc=97, loss=0.477]     \n",
      "FTLL Attack Epoch 2/10: 100%|██████████| 938/938 [01:39<00:00,  9.43it/s, acc=98, loss=2.03e-5]   \n",
      "FTLL Attack Epoch 3/10: 100%|██████████| 938/938 [01:39<00:00,  9.46it/s, acc=98.1, loss=2.05e-7] \n",
      "FTLL Attack Epoch 4/10: 100%|██████████| 938/938 [01:36<00:00,  9.70it/s, acc=98, loss=0.219]     \n",
      "FTLL Attack Epoch 5/10: 100%|██████████| 938/938 [01:39<00:00,  9.47it/s, acc=98.1, loss=2.72e-7] \n",
      "FTLL Attack Epoch 6/10: 100%|██████████| 938/938 [01:37<00:00,  9.62it/s, acc=98.1, loss=0.0051]  \n",
      "FTLL Attack Epoch 7/10: 100%|██████████| 938/938 [01:34<00:00,  9.91it/s, acc=98.1, loss=1.23]    \n",
      "FTLL Attack Epoch 8/10: 100%|██████████| 938/938 [01:39<00:00,  9.39it/s, acc=98, loss=1.21]      \n",
      "FTLL Attack Epoch 9/10: 100%|██████████| 938/938 [01:40<00:00,  9.35it/s, acc=97.9, loss=0.00153] \n",
      "FTLL Attack Epoch 10/10: 100%|██████████| 938/938 [01:42<00:00,  9.14it/s, acc=98.1, loss=2.24e-8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Executing attacks on FashionMNIST watermarked model ===\n",
      "\n",
      "1. Fine-Tune Last Layer (FTLL) Attack\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FTLL Attack Epoch 1/10: 100%|██████████| 938/938 [01:39<00:00,  9.46it/s, acc=88.5, loss=0.703] \n",
      "FTLL Attack Epoch 2/10: 100%|██████████| 938/938 [01:41<00:00,  9.24it/s, acc=89.5, loss=0.756] \n",
      "FTLL Attack Epoch 3/10: 100%|██████████| 938/938 [01:36<00:00,  9.69it/s, acc=89.7, loss=0.244] \n",
      "FTLL Attack Epoch 4/10: 100%|██████████| 938/938 [01:39<00:00,  9.43it/s, acc=89.9, loss=0.147] \n",
      "FTLL Attack Epoch 5/10: 100%|██████████| 938/938 [01:33<00:00, 10.03it/s, acc=90, loss=0.252]   \n",
      "FTLL Attack Epoch 6/10: 100%|██████████| 938/938 [01:41<00:00,  9.26it/s, acc=90.2, loss=0.512] \n",
      "FTLL Attack Epoch 7/10: 100%|██████████| 938/938 [01:36<00:00,  9.73it/s, acc=90, loss=0.352]   \n",
      "FTLL Attack Epoch 8/10: 100%|██████████| 938/938 [01:37<00:00,  9.66it/s, acc=90.1, loss=0.282] \n",
      "FTLL Attack Epoch 9/10: 100%|██████████| 938/938 [01:39<00:00,  9.45it/s, acc=90.2, loss=0.214] \n",
      "FTLL Attack Epoch 10/10: 100%|██████████| 938/938 [01:39<00:00,  9.44it/s, acc=90.2, loss=0.309] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def execute_attacks(watermarked_model, clean_dataloader, dataset_name):\n",
    "    \"\"\"Execute all four attack types on a watermarked model\"\"\"\n",
    "    print(f\"\\n=== Executing attacks on {dataset_name} watermarked model ===\")\n",
    "    \n",
    "    # Store original model state for fair comparison\n",
    "    original_state = copy.deepcopy(watermarked_model.state_dict())\n",
    "    \n",
    "    attacked_models = {}\n",
    "    \n",
    "    # FTLL Attack\n",
    "    print(\"\\n1. Fine-Tune Last Layer (FTLL) Attack\")\n",
    "    watermarked_model.load_state_dict(original_state)\n",
    "    attacked_models['FTLL'] = attack_ftll(watermarked_model, clean_dataloader)\n",
    "\n",
    "    \"\"\"\n",
    "    # FTAL Attack\n",
    "    print(\"\\n2. Fine-Tune All Layers (FTAL) Attack\")\n",
    "    watermarked_model.load_state_dict(original_state)\n",
    "    attacked_models['FTAL'] = attack_ftal(watermarked_model, clean_dataloader)\n",
    "    \n",
    "    # RTLL Attack\n",
    "    print(\"\\n3. Retrain Last Layer (RTLL) Attack\")\n",
    "    watermarked_model.load_state_dict(original_state)\n",
    "    attacked_models['RTLL'] = attack_rtll(watermarked_model, clean_dataloader)\n",
    "    \n",
    "    # RTAL Attack\n",
    "    print(\"\\n4. Retrain All Layers (RTAL) Attack\")\n",
    "    watermarked_model.load_state_dict(original_state)\n",
    "    attacked_models['RTAL'] = attack_rtal(watermarked_model, clean_dataloader)\n",
    "    \"\"\"\n",
    "    return attacked_models\n",
    "\n",
    "# Execute attacks on both datasets\n",
    "attacked_models_mnist = execute_attacks(watermarked_modelMNIST, clean_trainloaderMNIST, \"MNIST\")\n",
    "attacked_models_fashion = execute_attacks(watermarked_modelFashionMNIST, clean_trainloaderFashionMNIST, \"FashionMNIST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b703950e-a552-4a54-bd8b-9d59df9ef0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved FTLL attacked model: ./models/attacked/mnist_ftll_attacked.pth\n",
      "✓ Saved FTLL attacked model: ./models/attacked/fashionmnist_ftll_attacked.pth\n",
      "\n",
      "✓ All watermark removal attacks completed and models saved\n",
      "✓ Ready for evaluation in notebook 04_\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def save_attacked_models(attacked_models, dataset_name):\n",
    "    \"\"\"Save all attacked models for evaluation\"\"\"\n",
    "    os.makedirs('./models/attacked/', exist_ok=True)\n",
    "    \n",
    "    for attack_type, model in attacked_models.items():\n",
    "        model_path = f'./models/attacked/{dataset_name.lower()}_{attack_type.lower()}_attacked.pth'\n",
    "        torch.save(model, model_path)\n",
    "        print(f\"✓ Saved {attack_type} attacked model: {model_path}\")\n",
    "\n",
    "# Save all attacked models\n",
    "save_attacked_models(attacked_models_mnist, \"MNIST\")\n",
    "save_attacked_models(attacked_models_fashion, \"FashionMNIST\")\n",
    "\n",
    "print(\"\\n✓ All watermark removal attacks completed and models saved\")\n",
    "print(\"✓ Ready for evaluation in notebook 04_\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
